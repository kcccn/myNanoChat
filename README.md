> 主要参考：[Transformer Explainer: LLM Transformer Model Visually Explained](https://poloclub.github.io/transformer-explainer/)
>
> 感谢作者 *Aeree Cho, Grace C. Kim, Alexander Karpekov, Alec Helbling, Zijie J. Wang, Seongmin Lee, Benjamin Hoover, Duen Horng Chau* 在模型可视化方面的卓越工作，让我得以深入浅出地了解主流 LLM 的运作框架。
>
> 感谢 Gemini 以及各种 LLM，他/她/它们让我得以

# 0 前言

曾经听过一个很流行的论调“不要重复造轮子”，……

# 1 主要框架

# 2 Tokenizer

## 2.1 Tokenization

不同句子的数量是无限的，但是组成句子的词语却是可数的。以词语为单位处理句子明显比直接处理句子更加可行。因此，我们构造一个词表 $V$，词表包含了绝大部分可能用到的词语。句子 $S$ 进入模型前，先被拆分成若干个有序的词语：

$$
S = w_1 + w_2 + ... + w_n, \quad w_i \in V
$$

其中 "$+$" 表示词语的连接操作，$n$ 是句子 $S$ 中词语的数量，$w_i$ 是句子 $S$ 中的第 $i$ 个词语，且该词语属于词表 $V$。然后，每个词语 $w_i$ 被映射为一个唯一的整数 $t_i$，作为其编号。$w_i$ 便被称作 "token"，$t_i$ 则被称作 "token ID"。

举例：

$$
\text{句子 = “一分耕耘，一分收获。”} \\

\text{token = “一分” + “耕耘” + “，” + “一分” + “收获” + “。”} \\

\text{tokenID = [1, 2, 3, 1, 4, 5]}
$$

这个从句子到 tokenID 的映射过程，便是 **tokenization**（分词）。这个过程由 **Tokenizer**（分词器）完成。

## 2.2 双字节编码（Byte-Pair Encoding, BPE）

如何得到一个 GPT2 风格的 Tokenizer？主要分为 **构造词表** 和 **根据词表** 分词两个步骤。

### 2.2.1 词表构造

- 准备一个大规模的文本语料库 (corpus) 。初始时，可以将每个字符作为一个独立的 token，如 `a`, `b`, `c`, ..., `z`, ...。
- 统计语料库中所有相邻 token 对的出现频率。比如 `th`、`he`、`in` 等等。
- 选择出现频率最高的 token 对，将其合并为一个新的 token。
- 这样不断重复上述过程，每合并一次，就会生成一个新的 token （合并前的两个 token 不会被删除），直到达到预定的词表大小为止。

这样，我们就得到了一个包含常用词语的词表，可以更高效地表示文本。

> 值得注意的是，实际操作中，为了适配不同的语言，提高 Tokenizer 的泛化性，通常不直接使用字符作为初始 token，而是使用 **字节**。如 UTF-8 编码下 `爱` 对应 `\xe7\x88\xb1`，`w` 对应 `\x77`，则 `\xe7`、`\x88`、`\xb1`、`\x77` 都会被视为独立的初始 token。

### 2.2.2 分词过程

上一步中，我们已经构造出了模型的词表。进一步地，我们需要根据词表对输入文本进行分词，这是一个按秩合并的过程。我们这样定义词表中 token 的 "秩" (可以理解为权重或排名)，越早生成的 token 秩越小。然后，对于输入文本，我们可以列出所有的相邻 token 对，然后优先合并词表中存在的且秩最小的 token 对，直到无法继续合并为止。

这样的策略可以确保我们优先使用词表中更常用的 token，从而提高分词的效率和准确性。

> 其实分词和构造词表是非常相似的，只不过构造词表时是以频率为依据，而分词时是以秩为依据，还有一个区别是构造词表时会不断添加新的 token，而分词时不会。

## 2.3 算法实现

具体实现上，主要采取可删堆来高效维护当前相邻 token 对的信息。这里不再赘述，可以阅读[src/tokenizer.py](src/tokenizer.py) 了解具体细节。

